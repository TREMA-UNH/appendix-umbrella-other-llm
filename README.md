# appendix-umbrella-other-llm



This repository contains code and resources for reproducing the experiments in **"Does UMBRELA Work on Other LLMs?"**. Our study investigates the generalizability of the UMBRELA LLM Judge evaluation framework across different large language models (LLMs), assessing its effectiveness beyond the original study.

## ðŸ“Œ Overview
The UMBRELA framework provides a zero-shot, structured prompting approach for generating graded relevance labels. This work explores:
- How different LLMs impact relevance assessment accuracy.
- The effect of model scale on leaderboard rank correlation and per-label agreement.
- The reproducibility of UMBRELA's effectiveness across various LLM families.

## ðŸ“‚ Repository Structure
```
â”œâ”€â”€ prompts/ # Prompt templates for different settings 
â”‚ â”œâ”€â”€ qrel_fewshot_basic.txt 
â”‚ â”œâ”€â”€ qrel_fewshot_bing.txt 
â”‚ â”œâ”€â”€ qrel_zeroshot_basic.txt 
â”‚ â”œâ”€â”€ qrel_zeroshot_bing.txt 
â”œâ”€â”€ results/ # Stores evaluation outputs 
â”œâ”€â”€ src/ # Source code for processing and evaluation 
â”‚ â”œâ”€â”€ data_processing.py # Handles input/output
â”‚ â”œâ”€â”€ main.py # Main script to run the evaluation 
â”‚ â”œâ”€â”€ make_rubric_format.py # Converts data into rubric-based format 
â”‚ â”œâ”€â”€ model_utils.py # Functions to load and interact with LLMs 
â”‚ â”œâ”€â”€ prompts.py # Handles prompt construction 
â”‚ â”œâ”€â”€ relevance_processors.py 
â”‚ â”œâ”€â”€ relevance_scoring.py 
â”œâ”€â”€ run_command.sh # Example script for running the evaluation 
â””â”€â”€ README.md # Project documentation
```